#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ–‡ä»¶é”€å”®æ•°æ®å¤„ç†å™¨

æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªCSV/XLSXæ–‡ä»¶çš„é”€å”®æ•°æ®åˆ†æ
"""

import pandas as pd
import numpy as np
import os
import glob
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class MultiFileProcessor:
    """å¤šæ–‡ä»¶å¤„ç†å™¨ç±»"""
    
    def __init__(self):
        self.processed_files = {}
        self.combined_data = None
        self.file_summaries = {}
        
    def scan_directory(self, directory_path="data"):
        """æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰CSVå’ŒXLSXæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å¤¹ï¼‰"""
        files = []
        directory = Path(directory_path)
        
        if not directory.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        print(f"ğŸ” å¼€å§‹é€’å½’æ‰«æç›®å½•: {directory_path}")
        
        # é€’å½’æ‰«æCSVæ–‡ä»¶
        csv_files = list(directory.rglob("*.csv"))
        xlsx_files = list(directory.rglob("*.xlsx"))
        
        print(f"ğŸ“ å‘ç° CSV æ–‡ä»¶: {len(csv_files)} ä¸ª")
        print(f"ğŸ“ å‘ç° XLSX æ–‡ä»¶: {len(xlsx_files)} ä¸ª")
        
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶
        all_files = csv_files + xlsx_files
        
        # æ‰“å°æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        for file_path in sorted(all_files):
            relative_path = file_path.relative_to(directory)
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            print(f"  ğŸ“„ {relative_path} ({file_size:.2f} MB)")
        
        return sorted([str(f) for f in all_files])
    
    def load_file(self, file_path):
        """åŠ è½½å•ä¸ªæ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        try:
            file_path = Path(file_path)
            print(f"      â”Œâ”€ å¼€å§‹åŠ è½½æ–‡ä»¶: {file_path.name}")
            print(f"      â”œâ”€ æ–‡ä»¶å¤§å°: {file_path.stat().st_size / (1024 * 1024):.2f} MB")
            
            if file_path.suffix.lower() == '.csv':
                print(f"      â”œâ”€ æ–‡ä»¶ç±»å‹: CSV")
                # å°è¯•ä¸åŒçš„ç¼–ç 
                for encoding in ['utf-8', 'gbk', 'latin-1']:
                    try:
                        print(f"      â”œâ”€ å°è¯•ç¼–ç : {encoding}")
                        df = pd.read_csv(file_path, encoding=encoding)
                        print(f"      â”œâ”€ ç¼–ç æˆåŠŸ: {encoding}")
                        break
                    except UnicodeDecodeError as e:
                        print(f"      â”œâ”€ ç¼–ç å¤±è´¥: {encoding} - {str(e)}")
                        continue
                else:
                    raise Exception("æ— æ³•è¯»å–CSVæ–‡ä»¶ï¼Œç¼–ç é—®é¢˜")
                    
            elif file_path.suffix.lower() == '.xlsx':
                print(f"      â”œâ”€ æ–‡ä»¶ç±»å‹: XLSX")
                df = pd.read_excel(file_path)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
            
            print(f"      â”œâ”€ æ–‡ä»¶åŠ è½½æˆåŠŸ")
            print(f"      â”œâ”€ æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")
            print(f"      â””â”€ åˆ—å: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}")
            
            return df
            
        except Exception as e:
            print(f"      â””â”€ æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            raise Exception(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
    
    def standardize_columns(self, df, file_name):
        """æ ‡å‡†åŒ–åˆ—åï¼Œå°è¯•è¯†åˆ«é”€å”®æ•°æ®çš„å…³é”®åˆ—"""
        
        # å¸¸è§çš„åˆ—åæ˜ å°„
        column_mapping = {
            # è®¢å•IDç›¸å…³
            'order_id': 'Order_ID',
            'orderid': 'Order_ID', 
            'invoice': 'Order_ID',
            'invoiceno': 'Order_ID',
            'transaction_id': 'Order_ID',
            
            # äº§å“ç›¸å…³
            'product': 'Product',
            'product_name': 'Product',
            'description': 'Product',
            'stockcode': 'Product',
            'item': 'Product',
            
            # æ•°é‡ç›¸å…³
            'quantity': 'Quantity',
            'qty': 'Quantity',
            'amount': 'Quantity',
            
            # ä»·æ ¼ç›¸å…³
            'price': 'Price',
            'unit_price': 'Price',
            'unitprice': 'Price',
            'cost': 'Price',
            
            # æ—¥æœŸç›¸å…³
            'date': 'Order_Date',
            'order_date': 'Order_Date',
            'invoicedate': 'Order_Date',
            'transaction_date': 'Order_Date',
            
            # åœ°åŒºç›¸å…³
            'region': 'Region',
            'country': 'Region',
            'location': 'Region',
            'area': 'Region',
            
            # å®¢æˆ·ç›¸å…³
            'customer': 'Customer_ID',
            'customer_id': 'Customer_ID',
            'customerid': 'Customer_ID',
        }
        
        # è½¬æ¢åˆ—åä¸ºå°å†™è¿›è¡Œæ˜ å°„
        df_copy = df.copy()
        original_columns = df_copy.columns.tolist()
        
        # åˆ›å»ºæ–°çš„åˆ—åæ˜ å°„
        new_columns = {}
        for col in original_columns:
            col_lower = col.lower().strip()
            if col_lower in column_mapping:
                new_columns[col] = column_mapping[col_lower]
            else:
                new_columns[col] = col
                
        df_copy.rename(columns=new_columns, inplace=True)
        
        # æ·»åŠ æ–‡ä»¶æ¥æºæ ‡è¯†
        df_copy['Source_File'] = file_name
        
        return df_copy, new_columns
    
    def basic_data_cleaning(self, df):
        """åŸºç¡€æ•°æ®æ¸…æ´— - æŒ‰ç…§é¡¹ç›®éœ€æ±‚å®ç°å®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹"""
        df_clean = df.copy()
        print(f"    â”œâ”€ å¼€å§‹æ•°æ®æ¸…æ´—...")
        print(f"       â”œâ”€ æ¸…æ´—å‰è¡Œæ•°: {len(df_clean)}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé”€å”®æ•°æ®
        required_cols = ['Order_ID', 'Product', 'Quantity', 'Price', 'Order_Date', 'Region']
        has_sales_data = all(col in df_clean.columns for col in required_cols)
        
        if has_sales_data:
            print(f"       â”œâ”€ è¯†åˆ«ä¸ºé”€å”®æ•°æ®ï¼Œæ‰§è¡Œå®Œæ•´æ¸…æ´—æµç¨‹")
            
            # 1. å¤„ç†Priceç¼ºå¤±å€¼ - ç”¨ä¸­ä½æ•°å¡«å……
            price_missing_before = df_clean['Price'].isnull().sum()
            if price_missing_before > 0:
                price_median = df_clean['Price'].median()
                df_clean['Price'].fillna(price_median, inplace=True)
                print(f"       â”œâ”€ Priceåˆ—ç¼ºå¤±å€¼: {price_missing_before} -> 0 (ç”¨ä¸­ä½æ•° {price_median:.2f} å¡«å……)")
            
            # 2. å¤„ç†Regionç¼ºå¤±å€¼ - ç”¨"Unknown"å¡«å……
            region_missing_before = df_clean['Region'].isnull().sum()
            if region_missing_before > 0:
                df_clean['Region'].fillna('Unknown', inplace=True)
                print(f"       â”œâ”€ Regionåˆ—ç¼ºå¤±å€¼: {region_missing_before} -> 0 (ç”¨'Unknown'å¡«å……)")
            
            # 3. åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ
            rows_before = len(df_clean)
            df_clean.drop_duplicates(inplace=True)
            rows_after = len(df_clean)
            print(f"       â”œâ”€ åˆ é™¤é‡å¤è¡Œ: {rows_before} -> {rows_after} (åˆ é™¤ {rows_before - rows_after} è¡Œ)")
            
            # 4. å¤„ç†æ—¥æœŸåˆ—
            try:
                df_clean['Order_Date'] = pd.to_datetime(df_clean['Order_Date'], errors='coerce')
                print(f"       â”œâ”€ Order_Dateåˆ—å·²è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼")
            except Exception as e:
                print(f"       â”œâ”€ æ—¥æœŸè½¬æ¢è­¦å‘Š: {e}")
            
            # 5. å¼‚å¸¸å€¼å¤„ç† - IQRæ–¹æ³•å¤„ç†Quantityåˆ—
            Q1 = df_clean['Quantity'].quantile(0.25)
            Q3 = df_clean['Quantity'].quantile(0.75)
            IQR = Q3 - Q1
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_before = len(df_clean[df_clean['Quantity'] > upper_bound])
            df_clean.loc[df_clean['Quantity'] > upper_bound, 'Quantity'] = upper_bound
            outliers_after = len(df_clean[df_clean['Quantity'] > upper_bound])
            print(f"       â”œâ”€ Quantityå¼‚å¸¸å€¼å¤„ç†: {outliers_before} -> {outliers_after} (ä¸Šç•Œ: {upper_bound:.2f})")
            
            # 6. åˆ›å»ºSalesåˆ—
            df_clean['Sales'] = df_clean['Quantity'] * df_clean['Price']
            print(f"       â”œâ”€ å·²åˆ›å»ºSalesåˆ— (Quantity Ã— Price)")
            
            # 7. ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
            numeric_columns = ['Quantity', 'Price', 'Sales']
            for col in numeric_columns:
                if col in df_clean.columns:
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        else:
            print(f"       â”œâ”€ éé”€å”®æ•°æ®ï¼Œæ‰§è¡ŒåŸºç¡€æ¸…æ´—")
            # åŸºç¡€æ¸…æ´—é€»è¾‘
            # å°è¯•å¤„ç†æ—¥æœŸåˆ—
            date_columns = ['Order_Date', 'date', 'Date']
            for col in date_columns:
                if col in df_clean.columns:
                    try:
                        df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
                    except:
                        pass
            
            # å°è¯•å¤„ç†æ•°å€¼åˆ—
            numeric_columns = ['Quantity', 'Price', 'Sales']
            for col in numeric_columns:
                if col in df_clean.columns:
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        
        print(f"       â””â”€ æ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆè¡Œæ•°: {len(df_clean)}")
        return df_clean
    
    def process_single_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            print(f"    â”Œâ”€ å¼€å§‹å¤„ç†å•ä¸ªæ–‡ä»¶: {Path(file_path).name}")
            
            # åŠ è½½æ–‡ä»¶
            print(f"    â”œâ”€ æ­£åœ¨åŠ è½½æ–‡ä»¶...")
            df = self.load_file(file_path)
            file_name = Path(file_path).name
            print(f"    â”œâ”€ æ–‡ä»¶åŠ è½½å®Œæˆï¼ŒåŸå§‹æ•°æ®: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
            
            # æ ‡å‡†åŒ–åˆ—å
            print(f"    â”œâ”€ æ­£åœ¨æ ‡å‡†åŒ–åˆ—å...")
            df_std, column_mapping = self.standardize_columns(df, file_name)
            mapped_columns = [k for k, v in column_mapping.items() if k != v]
            if mapped_columns:
                print(f"    â”œâ”€ å·²æ˜ å°„åˆ—å: {len(mapped_columns)} ä¸ª")
            else:
                print(f"    â”œâ”€ æ— éœ€æ˜ å°„åˆ—å")
            
            # åŸºç¡€æ¸…æ´—
            print(f"    â”œâ”€ æ­£åœ¨è¿›è¡Œæ•°æ®æ¸…æ´—...")
            df_clean = self.basic_data_cleaning(df_std)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé”€å”®æ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–
            required_cols = ['Order_ID', 'Product', 'Quantity', 'Price', 'Order_Date', 'Region']
            has_sales_data = all(col in df_clean.columns for col in required_cols)
            
            visualization_files = []
            if has_sales_data and 'Sales' in df_clean.columns:
                print(f"    â”œâ”€ æ£€æµ‹åˆ°é”€å”®æ•°æ®ï¼Œç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
                visualization_files = self.generate_sales_visualization(df_clean)
            
            # æ£€æŸ¥Salesåˆ—
            if 'Sales' in df_clean.columns:
                sales_count = df_clean['Sales'].notna().sum()
                total_sales = df_clean['Sales'].sum()
                print(f"    â”œâ”€ é”€å”®æ•°æ®: {sales_count} æ¡æœ‰æ•ˆè®°å½•ï¼Œæ€»é¢: {total_sales:,.2f}")
            else:
                print(f"    â”œâ”€ æœªæ‰¾åˆ°é”€å”®æ•°æ®åˆ—")
            
            # ç”Ÿæˆæ–‡ä»¶æ‘˜è¦
            print(f"    â”œâ”€ æ­£åœ¨ç”Ÿæˆæ–‡ä»¶æ‘˜è¦...")
            summary = self.generate_file_summary(df_clean, file_name)
            
            # æ·»åŠ å¯è§†åŒ–æ–‡ä»¶ä¿¡æ¯åˆ°æ‘˜è¦
            if visualization_files:
                summary['visualization_files'] = visualization_files
            
            # å­˜å‚¨å¤„ç†ç»“æœ
            self.processed_files[file_name] = {
                'original_data': df,
                'processed_data': df_clean,
                'column_mapping': column_mapping,
                'summary': summary,
                'file_path': file_path,
                'visualization_files': visualization_files
            }
            
            print(f"    â””â”€ å•ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆ")
            
            return df_clean, summary
            
        except Exception as e:
            print(f"    â””â”€ å•ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            raise Exception(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
    
    def generate_file_summary(self, df, file_name):
        """ç”Ÿæˆæ–‡ä»¶æ‘˜è¦ç»Ÿè®¡ - å¢å¼ºé”€å”®æ•°æ®åˆ†æ"""
        summary = {
            'file_name': file_name,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'columns': df.columns.tolist(),
            'missing_values': df.isnull().sum().to_dict(),
            'data_types': df.dtypes.to_dict()
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé”€å”®æ•°æ®
        required_cols = ['Order_ID', 'Product', 'Quantity', 'Price', 'Order_Date', 'Region']
        has_sales_data = all(col in df.columns for col in required_cols)
        summary['is_sales_data'] = has_sales_data
        
        if has_sales_data:
            print(f"    â”œâ”€ ç”Ÿæˆé”€å”®æ•°æ®åˆ†ææ‘˜è¦...")
            
            # é”€å”®é¢ç»Ÿè®¡
            if 'Sales' in df.columns:
                summary['total_sales'] = float(df['Sales'].sum())
                summary['avg_sales'] = float(df['Sales'].mean())
                summary['median_sales'] = float(df['Sales'].median())
                summary['max_sales'] = float(df['Sales'].max())
                summary['min_sales'] = float(df['Sales'].min())
                print(f"       â”œâ”€ æ€»é”€å”®é¢: {summary['total_sales']:,.2f}")
                print(f"       â”œâ”€ å¹³å‡é”€å”®é¢: {summary['avg_sales']:,.2f}")
            
            # åœ°åŒºç»Ÿè®¡
            if 'Region' in df.columns:
                region_stats = df.groupby('Region').agg({
                    'Sales': ['sum', 'count', 'mean'] if 'Sales' in df.columns else 'count'
                }).round(2)
                
                if 'Sales' in df.columns:
                    region_stats.columns = ['æ€»é”€å”®é¢', 'è®¢å•æ•°', 'å¹³å‡è®¢å•é‡‘é¢']
                    summary['region_analysis'] = region_stats.to_dict('index')
                    print(f"       â”œâ”€ åœ°åŒºåˆ†æå®Œæˆï¼Œå…± {len(region_stats)} ä¸ªåœ°åŒº")
                
            # äº§å“ç»Ÿè®¡ - å¢å¼ºé”™è¯¯å¤„ç†
            if 'Product' in df.columns:
                try:
                    # æ£€æŸ¥Productåˆ—æ˜¯å¦æœ‰æ•ˆæ•°æ®
                    valid_products = df['Product'].notna()
                    if valid_products.sum() > 0:
                        # æ¸…ç†Productåˆ—æ•°æ®
                        df_clean_products = df[valid_products].copy()
                        
                        # ç¡®ä¿Productåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
                        df_clean_products['Product'] = df_clean_products['Product'].astype(str)
                        
                        # ç§»é™¤ç©ºå­—ç¬¦ä¸²å’Œ"nan"å­—ç¬¦ä¸² - ä¿®å¤å¸ƒå°”åˆ¤æ–­é”™è¯¯
                        # åˆ†æ­¥è¿›è¡Œæ¡ä»¶ç­›é€‰ä»¥é¿å…Serieså¸ƒå°”åˆ¤æ–­é”™è¯¯
                        mask1 = df_clean_products['Product'] != ''
                        mask2 = df_clean_products['Product'] != 'nan'
                        mask3 = df_clean_products['Product'] != 'None'
                        
                        # ç»„åˆæ‰€æœ‰æ¡ä»¶
                        combined_mask = mask1 & mask2 & mask3
                        df_clean_products = df_clean_products[combined_mask]
                        
                        if len(df_clean_products) > 0:
                            # é‡ç½®ç´¢å¼•ä»¥é¿å…groupbyé—®é¢˜
                            df_clean_products = df_clean_products.reset_index(drop=True)
                            
                            # è¿›è¡Œäº§å“åˆ†ç»„ç»Ÿè®¡
                            if 'Sales' in df.columns:
                                product_stats = df_clean_products.groupby('Product', as_index=False).agg({
                                    'Sales': ['sum', 'count']
                                }).round(2)
                                
                                # é‡æ–°å‘½ååˆ—
                                product_stats.columns = ['Product', 'æ€»é”€å”®é¢', 'è®¢å•æ•°']
                                product_stats = product_stats.set_index('Product')
                                
                                # åªä¿å­˜å‰10ä¸ªäº§å“
                                top_products = product_stats.sort_values('æ€»é”€å”®é¢', ascending=False).head(10)
                                summary['top_products'] = top_products.to_dict('index')
                                print(f"       â”œâ”€ äº§å“åˆ†æå®Œæˆï¼Œå…± {len(product_stats)} ä¸ªäº§å“")
                            else:
                                product_count = df_clean_products.groupby('Product', as_index=False).size()
                                print(f"       â”œâ”€ äº§å“åˆ†æå®Œæˆï¼Œå…± {len(product_count)} ä¸ªäº§å“ï¼ˆæ— é”€å”®æ•°æ®ï¼‰")
                        else:
                            print(f"       â”œâ”€ äº§å“åˆ—æ¸…ç†åæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡äº§å“åˆ†æ")
                    else:
                        print(f"       â”œâ”€ äº§å“åˆ—ä¸ºç©ºï¼Œè·³è¿‡äº§å“åˆ†æ")
                        
                except Exception as e:
                    print(f"       â”œâ”€ äº§å“åˆ†æé”™è¯¯: {str(e)}")
                    print(f"       â”œâ”€ è·³è¿‡äº§å“åˆ†æï¼Œç»§ç»­å¤„ç†å…¶ä»–ç»Ÿè®¡")
                    # ç»§ç»­å¤„ç†ï¼Œä¸ä¸­æ–­æµç¨‹
            
            # æ—¶é—´ç»´åº¦åˆ†æ
            if 'Order_Date' in df.columns:
                try:
                    date_col = pd.to_datetime(df['Order_Date'], errors='coerce')
                    valid_dates = date_col.dropna()
                    if len(valid_dates) > 0:
                        summary['date_range'] = {
                            'start': valid_dates.min().strftime('%Y-%m-%d'),
                            'end': valid_dates.max().strftime('%Y-%m-%d')
                        }
                        print(f"       â”œâ”€ æ—¥æœŸèŒƒå›´: {summary['date_range']['start']} ~ {summary['date_range']['end']}")
                except Exception as e:
                    print(f"       â”œâ”€ æ—¥æœŸåˆ†æè­¦å‘Š: {e}")
        else:
            print(f"    â”œâ”€ ç”ŸæˆåŸºç¡€æ•°æ®æ‘˜è¦...")
            # æ•°å€¼ç»Ÿè®¡
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                summary['numeric_summary'] = df[numeric_cols].describe().to_dict()
        
        return summary
    
    def generate_sales_visualization(self, df, output_dir="outputs"):
        """ç”Ÿæˆé”€å”®æ•°æ®å¯è§†åŒ–å›¾è¡¨"""
        import matplotlib.pyplot as plt
        import matplotlib.font_manager as fm
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé”€å”®æ•°æ®
        required_cols = ['Region', 'Sales']
        if not all(col in df.columns for col in required_cols):
            print(f"    â”œâ”€ è·³è¿‡å›¾è¡¨ç”Ÿæˆï¼šç¼ºå°‘å¿…è¦çš„é”€å”®æ•°æ®åˆ—")
            return []
        
        print(f"    â”œâ”€ å¼€å§‹ç”Ÿæˆé”€å”®æ•°æ®å¯è§†åŒ–å›¾è¡¨...")
        
        # é…ç½®ä¸­æ–‡å­—ä½“
        try:
            chinese_fonts = ['Microsoft YaHei', 'SimHei', 'SimSun', 'FangSong']
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            
            for font in chinese_fonts:
                if font in available_fonts:
                    plt.rcParams['font.sans-serif'] = [font]
                    plt.rcParams['axes.unicode_minus'] = False
                    break
            else:
                plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
                plt.rcParams['axes.unicode_minus'] = False
        except Exception as e:
            print(f"       â”œâ”€ å­—ä½“é…ç½®è­¦å‘Š: {e}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        generated_files = []
        
        try:
            # 1. å„åœ°åŒºæ€»é”€å”®é¢æŸ±çŠ¶å›¾
            plt.figure(figsize=(12, 8))
            region_sales = df.groupby('Region')['Sales'].sum().sort_values(ascending=False)
            
            bars = plt.bar(region_sales.index, region_sales.values, 
                          color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'],
                          alpha=0.8, edgecolor='black', linewidth=1)
            
            plt.title('å„åœ°åŒºæ€»é”€å”®é¢ (Total Sales by Region)', fontsize=16, fontweight='bold', pad=20)
            plt.xlabel('åœ°åŒº (Region)', fontsize=12, fontweight='bold')
            plt.ylabel('æ€»é”€å”®é¢ (Total Sales)', fontsize=12, fontweight='bold')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (region, value) in enumerate(region_sales.items()):
                plt.text(i, value + max(region_sales.values) * 0.01, 
                        f'{value:,.0f}', ha='center', va='bottom', 
                        fontweight='bold', fontsize=10)
            
            plt.tight_layout()
            chart_file = f"{output_dir}/sales_by_region.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            generated_files.append(chart_file)
            print(f"       â”œâ”€ ç”ŸæˆæŸ±çŠ¶å›¾: {chart_file}")
            
            # 2. ç»¼åˆåˆ†æå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('é”€å”®æ•°æ®ç»¼åˆåˆ†æ (Comprehensive Sales Analysis)', 
                        fontsize=16, fontweight='bold', y=0.98)
            
            # 2.1 å„åœ°åŒºé”€å”®é¢åˆ†å¸ƒ
            region_sales.plot(kind='bar', ax=axes[0, 0], color='skyblue', alpha=0.8)
            axes[0, 0].set_title('å„åœ°åŒºæ€»é”€å”®é¢ (Total Sales by Region)', fontweight='bold')
            axes[0, 0].set_xlabel('åœ°åŒº (Region)')
            axes[0, 0].set_ylabel('æ€»é”€å”®é¢ (Total Sales)')
            axes[0, 0].tick_params(axis='x', rotation=45)
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2.2 é”€å”®é¢åˆ†å¸ƒç›´æ–¹å›¾
            axes[0, 1].hist(df['Sales'], bins=30, color='lightcoral', alpha=0.7, edgecolor='black')
            axes[0, 1].set_title('é”€å”®é¢åˆ†å¸ƒ (Sales Distribution)', fontweight='bold')
            axes[0, 1].set_xlabel('é”€å”®é¢ (Sales Amount)')
            axes[0, 1].set_ylabel('é¢‘æ¬¡ (Frequency)')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 2.3 äº§å“é”€å”®é¢å‰10
            if 'Product' in df.columns:
                try:
                    # æ£€æŸ¥Productåˆ—æ˜¯å¦æœ‰æ•ˆ
                    valid_products = df['Product'].notna()
                    if valid_products.sum() > 0:
                        # æ¸…ç†Productåˆ—æ•°æ®
                        df_clean_products = df[valid_products].copy()
                        
                        # ç¡®ä¿Productåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
                        df_clean_products['Product'] = df_clean_products['Product'].astype(str)
                        
                        # ç§»é™¤ç©ºå­—ç¬¦ä¸²å’Œ"nan"å­—ç¬¦ä¸² - ä¿®å¤å¸ƒå°”åˆ¤æ–­é”™è¯¯
                        # åˆ†æ­¥è¿›è¡Œæ¡ä»¶ç­›é€‰ä»¥é¿å…Serieså¸ƒå°”åˆ¤æ–­é”™è¯¯
                        mask1 = df_clean_products['Product'] != ''
                        mask2 = df_clean_products['Product'] != 'nan'
                        mask3 = df_clean_products['Product'] != 'None'
                        
                        # ç»„åˆæ‰€æœ‰æ¡ä»¶
                        combined_mask = mask1 & mask2 & mask3
                        df_clean_products = df_clean_products[combined_mask]
                        
                        if len(df_clean_products) > 0 and 'Sales' in df_clean_products.columns:
                            # é‡ç½®ç´¢å¼•ä»¥é¿å…groupbyé—®é¢˜
                            df_clean_products = df_clean_products.reset_index(drop=True)
                            
                            # è¿›è¡Œäº§å“åˆ†ç»„ç»Ÿè®¡
                            product_sales = df_clean_products.groupby('Product', as_index=False)['Sales'].sum()
                            product_sales = product_sales.sort_values('Sales', ascending=False).head(10)
                            
                            if len(product_sales) > 0:
                                # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
                                y_pos = range(len(product_sales))
                                axes[1, 0].barh(y_pos, product_sales['Sales'], 
                                               color='lightgreen', alpha=0.8)
                                axes[1, 0].set_yticks(y_pos)
                                axes[1, 0].set_yticklabels(product_sales['Product'], fontsize=9)
                                axes[1, 0].set_title('é”€å”®é¢æœ€é«˜çš„å‰10ä¸ªäº§å“ (Top 10 Products by Sales)', fontweight='bold')
                                axes[1, 0].set_xlabel('æ€»é”€å”®é¢ (Total Sales)')
                                axes[1, 0].grid(True, alpha=0.3, axis='x')
                            else:
                                # å¦‚æœæ²¡æœ‰äº§å“æ•°æ®ï¼Œæ˜¾ç¤ºå ä½å›¾
                                axes[1, 0].text(0.5, 0.5, 'æ— äº§å“æ•°æ®\n(No Product Data)', 
                                               transform=axes[1, 0].transAxes, ha='center', va='center',
                                               fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                                axes[1, 0].set_title('äº§å“é”€å”®åˆ†æ (Product Sales Analysis)', fontweight='bold')
                        else:
                            # å¦‚æœæ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®
                            axes[1, 0].text(0.5, 0.5, 'äº§å“æ•°æ®æ— æ•ˆ\n(Invalid Product Data)', 
                                           transform=axes[1, 0].transAxes, ha='center', va='center',
                                           fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                            axes[1, 0].set_title('äº§å“é”€å”®åˆ†æ (Product Sales Analysis)', fontweight='bold')
                    else:
                        # Productåˆ—å…¨ä¸ºç©ºå€¼
                        axes[1, 0].text(0.5, 0.5, 'äº§å“åˆ—ä¸ºç©º\n(Product Column Empty)', 
                                       transform=axes[1, 0].transAxes, ha='center', va='center',
                                       fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                        axes[1, 0].set_title('äº§å“é”€å”®åˆ†æ (Product Sales Analysis)', fontweight='bold')
                        
                except Exception as e:
                    print(f"       â”œâ”€ äº§å“åˆ†æè­¦å‘Š: {str(e)}")
                    # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                    axes[1, 0].text(0.5, 0.5, f'äº§å“åˆ†æé”™è¯¯\n{str(e)[:50]}...', 
                                   transform=axes[1, 0].transAxes, ha='center', va='center',
                                   fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"))
                    axes[1, 0].set_title('äº§å“é”€å”®åˆ†æ (Product Sales Analysis)', fontweight='bold')
            else:
                # æ²¡æœ‰Productåˆ—
                axes[1, 0].text(0.5, 0.5, 'ç¼ºå°‘äº§å“åˆ—\n(Missing Product Column)', 
                               transform=axes[1, 0].transAxes, ha='center', va='center',
                               fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                axes[1, 0].set_title('äº§å“é”€å”®åˆ†æ (Product Sales Analysis)', fontweight='bold')
            
            # 2.4 è®¢å•æ•°é‡vsé”€å”®é¢æ•£ç‚¹å›¾
            region_summary = df.groupby('Region').agg({'Sales': 'sum', 'Order_ID': 'count'})
            scatter = axes[1, 1].scatter(region_summary['Order_ID'], region_summary['Sales'], 
                                       s=120, alpha=0.7, color='orange', edgecolors='black')
            for i, region in enumerate(region_summary.index):
                axes[1, 1].annotate(region, 
                                   (region_summary['Order_ID'].iloc[i], region_summary['Sales'].iloc[i]),
                                   xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold')
            axes[1, 1].set_title('è®¢å•æ•°é‡ vs æ€»é”€å”®é¢ (Order Count vs Total Sales)', fontweight='bold')
            axes[1, 1].set_xlabel('è®¢å•æ•°é‡ (Order Count)')
            axes[1, 1].set_ylabel('æ€»é”€å”®é¢ (Total Sales)')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            comprehensive_file = f"{output_dir}/comprehensive_analysis.png"
            plt.savefig(comprehensive_file, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            generated_files.append(comprehensive_file)
            print(f"       â”œâ”€ ç”Ÿæˆç»¼åˆåˆ†æå›¾: {comprehensive_file}")
            
        except Exception as e:
            print(f"       â”œâ”€ å›¾è¡¨ç”Ÿæˆè­¦å‘Š: {e}")
        
        print(f"    â””â”€ å›¾è¡¨ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶")
        return generated_files

    def generate_analysis_report(self, output_dir="outputs"):
        """ç”Ÿæˆå®Œæ•´çš„é”€å”®æ•°æ®åˆ†ææŠ¥å‘Š"""
        if not self.processed_files:
            print("æ²¡æœ‰å·²å¤„ç†çš„æ–‡ä»¶æ•°æ®")
            return None
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå¹¶æ‰€æœ‰é”€å”®æ•°æ®
        sales_files = []
        for file_name, file_data in self.processed_files.items():
            if file_data['summary'].get('is_sales_data', False):
                sales_files.append((file_name, file_data))
        
        if not sales_files:
            print("æ²¡æœ‰å‘ç°é”€å”®æ•°æ®æ–‡ä»¶")
            return None
        
        print(f"å¼€å§‹ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œå…± {len(sales_files)} ä¸ªé”€å”®æ•°æ®æ–‡ä»¶")
        
        # åˆ›å»ºæŠ¥å‘Šå†…å®¹
        report_content = []
        report_content.append("# é”€å”®æ•°æ®åˆ†ææŠ¥å‘Š (Sales Data Analysis Report)")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("")
        report_content.append("## æ•°æ®æ¦‚è§ˆ (Data Overview)")
        
        total_records = 0
        total_sales = 0
        all_regions = set()
        all_products = set()
        
        for file_name, file_data in sales_files:
            summary = file_data['summary']
            total_records += summary['total_rows']
            total_sales += summary.get('total_sales', 0)
            
            # æ”¶é›†åœ°åŒºå’Œäº§å“ä¿¡æ¯
            if 'region_analysis' in summary:
                all_regions.update(summary['region_analysis'].keys())
            if 'top_products' in summary:
                all_products.update(summary['top_products'].keys())
        
        report_content.append(f"- æ€»è®°å½•æ•°: {total_records:,}")
        report_content.append(f"- æ€»é”€å”®é¢: {total_sales:,.2f}")
        report_content.append(f"- æ¶‰åŠåœ°åŒº: {len(all_regions)} ä¸ª")
        report_content.append(f"- æ¶‰åŠäº§å“: {len(all_products)} ä¸ª")
        report_content.append("")
        
        # è¯¦ç»†åˆ†ææ¯ä¸ªæ–‡ä»¶
        for i, (file_name, file_data) in enumerate(sales_files, 1):
            summary = file_data['summary']
            report_content.append(f"## æ–‡ä»¶ {i}: {file_name}")
            report_content.append("")
            
            # åŸºæœ¬ä¿¡æ¯
            report_content.append("### åŸºæœ¬ä¿¡æ¯ (Basic Information)")
            report_content.append(f"- æ•°æ®è¡Œæ•°: {summary['total_rows']:,}")
            report_content.append(f"- æ•°æ®åˆ—æ•°: {summary['total_columns']}")
            report_content.append(f"- æ€»é”€å”®é¢: {summary.get('total_sales', 0):,.2f}")
            report_content.append(f"- å¹³å‡é”€å”®é¢: {summary.get('avg_sales', 0):,.2f}")
            if 'date_range' in summary:
                report_content.append(f"- æ—¥æœŸèŒƒå›´: {summary['date_range']['start']} ~ {summary['date_range']['end']}")
            report_content.append("")
            
            # åœ°åŒºåˆ†æ
            if 'region_analysis' in summary:
                report_content.append("### åœ°åŒºåˆ†æ (Regional Analysis)")
                region_data = summary['region_analysis']
                for region, stats in region_data.items():
                    report_content.append(f"- **{region}**: æ€»é”€å”®é¢ {stats['æ€»é”€å”®é¢']:,.2f}, è®¢å•æ•° {stats['è®¢å•æ•°']}, å¹³å‡è®¢å•é‡‘é¢ {stats['å¹³å‡è®¢å•é‡‘é¢']:,.2f}")
                report_content.append("")
            
            # äº§å“åˆ†æ
            if 'top_products' in summary:
                report_content.append("### çƒ­é”€äº§å“ (Top Products)")
                product_data = summary['top_products']
                for product, stats in list(product_data.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report_content.append(f"- **{product}**: æ€»é”€å”®é¢ {stats['æ€»é”€å”®é¢']:,.2f}, è®¢å•æ•° {stats['è®¢å•æ•°']}")
                report_content.append("")
            
            # å¯è§†åŒ–æ–‡ä»¶
            if 'visualization_files' in summary and summary['visualization_files']:
                report_content.append("### ç”Ÿæˆçš„å›¾è¡¨ (Generated Charts)")
                for chart_file in summary['visualization_files']:
                    chart_name = os.path.basename(chart_file)
                    report_content.append(f"- {chart_name}")
                report_content.append("")
        
        # ä¸šåŠ¡è§è§£
        report_content.append("## ä¸šåŠ¡è§è§£ (Business Insights)")
        report_content.append("")
        report_content.append("### ä¸»è¦å‘ç° (Key Findings)")
        
        # åœ°åŒºåˆ†æè§è§£
        if sales_files:
            all_region_data = {}
            for file_name, file_data in sales_files:
                if 'region_analysis' in file_data['summary']:
                    for region, stats in file_data['summary']['region_analysis'].items():
                        if region not in all_region_data:
                            all_region_data[region] = {'æ€»é”€å”®é¢': 0, 'è®¢å•æ•°': 0}
                        all_region_data[region]['æ€»é”€å”®é¢'] += stats['æ€»é”€å”®é¢']
                        all_region_data[region]['è®¢å•æ•°'] += stats['è®¢å•æ•°']
            
            if all_region_data:
                # æ‰¾å‡ºæœ€ä½³åœ°åŒº
                best_region = max(all_region_data.items(), key=lambda x: x[1]['æ€»é”€å”®é¢'])
                report_content.append(f"1. **æœ€ä½³é”€å”®åœ°åŒº**: {best_region[0]}ï¼Œæ€»é”€å”®é¢ä¸º {best_region[1]['æ€»é”€å”®é¢']:,.2f}")
                
                # è®¡ç®—åœ°åŒºåˆ†å¸ƒ
                total_region_sales = sum(data['æ€»é”€å”®é¢'] for data in all_region_data.values())
                best_region_percentage = (best_region[1]['æ€»é”€å”®é¢'] / total_region_sales) * 100
                report_content.append(f"   - å æ€»é”€å”®é¢çš„ {best_region_percentage:.1f}%")
                
                # æ‰¾å‡ºè®¢å•æ•°æœ€å¤šçš„åœ°åŒº
                most_orders_region = max(all_region_data.items(), key=lambda x: x[1]['è®¢å•æ•°'])
                if most_orders_region[0] != best_region[0]:
                    report_content.append(f"2. **è®¢å•æ•°æœ€å¤šåœ°åŒº**: {most_orders_region[0]}ï¼Œå…± {most_orders_region[1]['è®¢å•æ•°']} ä¸ªè®¢å•")
                
                # å¹³å‡è®¢å•é‡‘é¢åˆ†æ
                avg_order_values = {region: data['æ€»é”€å”®é¢'] / data['è®¢å•æ•°'] 
                                  for region, data in all_region_data.items() if data['è®¢å•æ•°'] > 0}
                if avg_order_values:
                    best_avg_region = max(avg_order_values.items(), key=lambda x: x[1])
                    report_content.append(f"3. **å¹³å‡è®¢å•é‡‘é¢æœ€é«˜åœ°åŒº**: {best_avg_region[0]}ï¼Œå¹³å‡ {best_avg_region[1]:,.2f}")
        
        report_content.append("")
        report_content.append("### å»ºè®® (Recommendations)")
        report_content.append("1. åŠ å¼ºåœ¨é«˜é”€å”®é¢åœ°åŒºçš„å¸‚åœºæŠ•å…¥ï¼Œå·©å›ºå¸‚åœºåœ°ä½")
        report_content.append("2. åˆ†æä½é”€å”®é¢åœ°åŒºçš„åŸå› ï¼Œåˆ¶å®šé’ˆå¯¹æ€§çš„æ”¹è¿›ç­–ç•¥")
        report_content.append("3. ç ”ç©¶é«˜å¹³å‡è®¢å•é‡‘é¢åœ°åŒºçš„æˆåŠŸå› ç´ ï¼Œæ¨å¹¿åˆ°å…¶ä»–åœ°åŒº")
        report_content.append("4. ä¼˜åŒ–äº§å“ç»“æ„ï¼Œé‡ç‚¹æ¨å¹¿çƒ­é”€äº§å“")
        report_content.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"{output_dir}/sales_analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report_file
    
    def process_multiple_files(self, file_paths, progress_callback=None):
        """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        results = {}
        total_files = len(file_paths)
        
        for i, file_path in enumerate(file_paths, 1):
            try:
                if progress_callback:
                    progress_callback(i-1, total_files, f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{total_files}: {Path(file_path).name}")
                
                print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{total_files}: {Path(file_path).name}")
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                df_processed, summary = self.process_single_file(file_path)
                
                results[Path(file_path).name] = {
                    'success': True,
                    'data': df_processed,
                    'summary': summary,
                    'file_path': file_path
                }
                
                print(f"âœ… æ–‡ä»¶ {i} å¤„ç†æˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ æ–‡ä»¶ {i} å¤„ç†å¤±è´¥: {str(e)}")
                results[Path(file_path).name] = {
                    'success': False,
                    'error': str(e),
                    'file_path': file_path
                }
        
        if progress_callback:
            progress_callback(total_files, total_files, "æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")
        
        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {sum(1 for r in results.values() if r.get('success', False))}/{total_files} ä¸ªæ–‡ä»¶")
        return results
    
    def combine_all_data(self):
        """åˆå¹¶æ‰€æœ‰å·²å¤„ç†çš„æ•°æ®"""
        if not self.processed_files:
            print("æ²¡æœ‰å·²å¤„ç†çš„æ•°æ®å¯ä»¥åˆå¹¶")
            return None
        
        print(f"\nğŸ”— å¼€å§‹åˆå¹¶æ•°æ®ï¼Œå…± {len(self.processed_files)} ä¸ªæ–‡ä»¶")
        
        combined_data_list = []
        
        for file_name, file_data in self.processed_files.items():
            if file_data.get('success', True):  # å…¼å®¹ä¸åŒçš„æ•°æ®ç»“æ„
                # è·å–å¤„ç†åçš„æ•°æ®
                if 'processed_data' in file_data:
                    df = file_data['processed_data']
                elif 'data' in file_data:
                    df = file_data['data']
                else:
                    print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file_name}ï¼šæ‰¾ä¸åˆ°å¤„ç†åçš„æ•°æ®")
                    continue
                
                if df is not None and len(df) > 0:
                    # åˆ›å»ºæ•°æ®å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                    df_copy = df.copy()
                    
                    # é‡ç½®ç´¢å¼•ä»¥é¿å…ç´¢å¼•å†²çª
                    df_copy = df_copy.reset_index(drop=True)
                    
                    # ç¡®ä¿åˆ—åå”¯ä¸€ä¸”ä¸€è‡´
                    if df_copy.columns.duplicated().any():
                        print(f"   â”œâ”€ ä¿®å¤æ–‡ä»¶ {file_name} çš„é‡å¤åˆ—å")
                        # ä¸ºé‡å¤åˆ—åæ·»åŠ åç¼€
                        cols = df_copy.columns.tolist()
                        seen = set()
                        for i, col in enumerate(cols):
                            if col in seen:
                                counter = 1
                                new_col = f"{col}_{counter}"
                                while new_col in seen:
                                    counter += 1
                                    new_col = f"{col}_{counter}"
                                cols[i] = new_col
                            seen.add(cols[i])
                        df_copy.columns = cols
                    
                    combined_data_list.append(df_copy)
                    print(f"   â”œâ”€ æ·»åŠ æ–‡ä»¶: {file_name} ({len(df_copy)} è¡Œ, {len(df_copy.columns)} åˆ—)")
                else:
                    print(f"   â”œâ”€ è·³è¿‡ç©ºæ–‡ä»¶: {file_name}")
        
        if not combined_data_list:
            print("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä»¥åˆå¹¶")
            return None
        
        try:
            # æ£€æŸ¥æ‰€æœ‰DataFrameçš„åˆ—ç»“æ„
            all_columns = set()
            for df in combined_data_list:
                all_columns.update(df.columns)
            
            print(f"   â”œâ”€ å‘ç°æ€»å…± {len(all_columns)} ä¸ªå”¯ä¸€åˆ—å")
            
            # ç»Ÿä¸€åˆ—ç»“æ„ - ä¸ºæ¯ä¸ªDataFrameæ·»åŠ ç¼ºå¤±çš„åˆ—
            standardized_dfs = []
            for i, df in enumerate(combined_data_list):
                df_std = df.copy()
                
                # æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆç”¨NaNå¡«å……ï¼‰
                missing_cols = all_columns - set(df.columns)
                for col in missing_cols:
                    df_std[col] = np.nan
                
                # é‡æ–°æ’åºåˆ—ä»¥ä¿æŒä¸€è‡´æ€§
                df_std = df_std.reindex(columns=sorted(all_columns))
                standardized_dfs.append(df_std)
            
            # åˆå¹¶æ•°æ®
            self.combined_data = pd.concat(standardized_dfs, ignore_index=True, sort=False)
            print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼Œåˆå¹¶åæ€»è¡Œæ•°: {len(self.combined_data)}")
            print(f"   â””â”€ åˆå¹¶ååˆ—æ•°: {len(self.combined_data.columns)}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            print(f"   â””â”€ å°è¯•ä½¿ç”¨å¤‡ç”¨åˆå¹¶æ–¹æ³•...")
            
            try:
                # å¤‡ç”¨æ–¹æ³•ï¼šåªåˆå¹¶å…·æœ‰ç›¸åŒåˆ—ç»“æ„çš„æ•°æ®
                grouped_by_columns = {}
                for i, df in enumerate(combined_data_list):
                    col_signature = tuple(sorted(df.columns))
                    if col_signature not in grouped_by_columns:
                        grouped_by_columns[col_signature] = []
                    grouped_by_columns[col_signature].append(df)
                
                # åˆ†åˆ«åˆå¹¶æ¯ç»„
                merged_groups = []
                for col_signature, df_group in grouped_by_columns.items():
                    if len(df_group) > 1:
                        group_merged = pd.concat(df_group, ignore_index=True)
                        print(f"   â”œâ”€ åˆå¹¶åˆ—ç»“æ„ç»„ {len(col_signature)} åˆ—: {len(df_group)} ä¸ªæ–‡ä»¶")
                    else:
                        group_merged = df_group[0]
                        print(f"   â”œâ”€ å•ä¸ªæ–‡ä»¶ç»„ {len(col_signature)} åˆ—: 1 ä¸ªæ–‡ä»¶")
                    merged_groups.append(group_merged)
                
                # æœ€ç»ˆåˆå¹¶æ‰€æœ‰ç»„ï¼ˆä½¿ç”¨outer joinï¼‰
                if len(merged_groups) == 1:
                    self.combined_data = merged_groups[0]
                else:
                    # ä½¿ç”¨å¤–è¿æ¥åˆå¹¶ä¸åŒç»“æ„çš„æ•°æ®
                    self.combined_data = merged_groups[0]
                    for df in merged_groups[1:]:
                        # æ‰¾åˆ°å…±åŒåˆ—
                        common_cols = list(set(self.combined_data.columns) & set(df.columns))
                        if common_cols:
                            print(f"   â”œâ”€ åŸºäº {len(common_cols)} ä¸ªå…±åŒåˆ—åˆå¹¶")
                            # åªä¿ç•™å…±åŒåˆ—è¿›è¡Œåˆå¹¶
                            df1_common = self.combined_data[common_cols].copy()
                            df2_common = df[common_cols].copy()
                            self.combined_data = pd.concat([df1_common, df2_common], ignore_index=True)
                        else:
                            print(f"   â”œâ”€ æ— å…±åŒåˆ—ï¼Œåˆ†åˆ«ä¿å­˜")
                            # å¦‚æœæ²¡æœ‰å…±åŒåˆ—ï¼Œæ·»åŠ æ‰€æœ‰åˆ—å¹¶ç”¨NaNå¡«å……
                            all_cols = list(set(self.combined_data.columns) | set(df.columns))
                            
                            # ä¸ºç¬¬ä¸€ä¸ªDataFrameæ·»åŠ ç¼ºå¤±åˆ—
                            for col in df.columns:
                                if col not in self.combined_data.columns:
                                    self.combined_data[col] = np.nan
                            
                            # ä¸ºç¬¬äºŒä¸ªDataFrameæ·»åŠ ç¼ºå¤±åˆ—
                            df_extended = df.copy()
                            for col in self.combined_data.columns:
                                if col not in df_extended.columns:
                                    df_extended[col] = np.nan
                            
                            # é‡æ–°æ’åºåˆ—
                            self.combined_data = self.combined_data.reindex(columns=all_cols)
                            df_extended = df_extended.reindex(columns=all_cols)
                            
                            # åˆå¹¶
                            self.combined_data = pd.concat([self.combined_data, df_extended], ignore_index=True)
                
                print(f"âœ… å¤‡ç”¨æ–¹æ³•åˆå¹¶æˆåŠŸï¼Œåˆå¹¶åæ€»è¡Œæ•°: {len(self.combined_data)}")
                print(f"   â””â”€ åˆå¹¶ååˆ—æ•°: {len(self.combined_data.columns)}")
                
            except Exception as backup_error:
                print(f"âŒ å¤‡ç”¨åˆå¹¶æ–¹æ³•ä¹Ÿå¤±è´¥: {str(backup_error)}")
                print(f"   â””â”€ è¿”å›ç©ºç»“æœ")
                return None
        
        return self.combined_data
    
    def save_results(self, output_dir="outputs", separate_files=True, combined_file=True):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        import os
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ å¼€å§‹ä¿å­˜ç»“æœåˆ°ç›®å½•: {output_dir}")
        print(f"ä¿å­˜è®¾ç½®: å•ç‹¬æ–‡ä»¶={separate_files}, åˆå¹¶æ–‡ä»¶={combined_file}")
        
        saved_files = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å•ç‹¬çš„å¤„ç†æ–‡ä»¶
        if separate_files and self.processed_files:
            print(f"   â”œâ”€ ä¿å­˜å•ç‹¬å¤„ç†æ–‡ä»¶...")
            
            for file_name, file_data in self.processed_files.items():
                if file_data.get('success', True):
                    # è·å–å¤„ç†åçš„æ•°æ®
                    if 'processed_data' in file_data:
                        df = file_data['processed_data']
                    elif 'data' in file_data:
                        df = file_data['data']
                    else:
                        continue
                    
                    if df is not None and len(df) > 0:
                        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                        base_name = Path(file_name).stem
                        output_file = f"{output_dir}/{base_name}_processed_{timestamp}.csv"
                        
                        # ä¿å­˜æ–‡ä»¶
                        df.to_csv(output_file, index=False, encoding='utf-8-sig')
                        saved_files.append(output_file)
                        
                        file_size = os.path.getsize(output_file) / 1024  # KB
                        print(f"      â”œâ”€ ä¿å­˜æ–‡ä»¶: {output_file} ({file_size:.1f} KB)")
        
        # ä¿å­˜åˆå¹¶æ–‡ä»¶
        if combined_file and self.combined_data is not None:
            print(f"   â”œâ”€ ä¿å­˜åˆå¹¶æ–‡ä»¶...")
            combined_output_file = f"{output_dir}/combined_data_{timestamp}.csv"
            self.combined_data.to_csv(combined_output_file, index=False, encoding='utf-8-sig')
            saved_files.append(combined_output_file)
            
            file_size = os.path.getsize(combined_output_file) / 1024  # KB
            print(f"      â”œâ”€ ä¿å­˜åˆå¹¶æ–‡ä»¶: {combined_output_file} ({file_size:.1f} KB)")
        
        # ä¿å­˜å¤„ç†æ‘˜è¦
        if self.processed_files:
            print(f"   â”œâ”€ ä¿å­˜å¤„ç†æ‘˜è¦...")
            summary_data = []
            
            for file_name, file_data in self.processed_files.items():
                if file_data.get('success', True):
                    summary = file_data.get('summary', {})
                    summary_data.append({
                        'file_name': file_name,
                        'total_rows': summary.get('total_rows', 0),
                        'total_columns': summary.get('total_columns', 0),
                        'is_sales_data': summary.get('is_sales_data', False),
                        'total_sales': summary.get('total_sales', 0),
                        'avg_sales': summary.get('avg_sales', 0)
                    })
            
            if summary_data:
                summary_df = pd.DataFrame(summary_data)
                summary_file = f"{output_dir}/processing_summary_{timestamp}.csv"
                summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
                saved_files.append(summary_file)
                
                file_size = os.path.getsize(summary_file) / 1024  # KB
                print(f"      â”œâ”€ ä¿å­˜æ‘˜è¦æ–‡ä»¶: {summary_file} ({file_size:.1f} KB)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå¦‚æœæœ‰é”€å”®æ•°æ®ï¼‰
        has_sales_data = any(
            file_data.get('summary', {}).get('is_sales_data', False) 
            for file_data in self.processed_files.values() 
            if file_data.get('success', True)
        )
        
        if has_sales_data:
            print(f"   â”œâ”€ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            try:
                report_file = self.generate_analysis_report(output_dir)
                if report_file:
                    saved_files.append(report_file)
                    file_size = os.path.getsize(report_file) / 1024  # KB
                    print(f"      â”œâ”€ ä¿å­˜åˆ†ææŠ¥å‘Š: {report_file} ({file_size:.1f} KB)")
            except Exception as e:
                print(f"      â”œâ”€ ç”Ÿæˆåˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        
        print(f"âœ… ç»“æœä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºä¿å­˜ç»“æœæ‘˜è¦
        total_size = sum(os.path.getsize(f) / 1024 for f in saved_files if os.path.exists(f))
        print(f"   â””â”€ æ–‡ä»¶æ€»å¤§å°: {total_size:.1f} KB")
        
        return saved_files
    
    def generate_combined_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆç”¨äºGUIï¼‰"""
        if not self.processed_files:
            return {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = len(self.processed_files)
        success_files = sum(1 for r in self.processed_files.values() if r.get('success', True))
        total_records = 0
        total_sales = 0
        
        # æ”¶é›†åœ°åŒºå’Œäº§å“ä¿¡æ¯
        all_regions = {}
        all_products = {}
        file_breakdown = {}
        
        for file_name, file_data in self.processed_files.items():
            if file_data.get('success', True):
                summary = file_data.get('summary', {})
                
                # åŸºæœ¬ç»Ÿè®¡
                records = summary.get('total_rows', 0)
                sales = summary.get('total_sales', 0)
                total_records += records
                total_sales += sales
                
                # æ–‡ä»¶åˆ†è§£
                file_breakdown[file_name] = {
                    'è®°å½•æ•°': records,
                    'é”€å”®é¢': sales,
                    'æ˜¯å¦é”€å”®æ•°æ®': summary.get('is_sales_data', False)
                }
                
                # åœ°åŒºç»Ÿè®¡
                if 'region_analysis' in summary:
                    for region, stats in summary['region_analysis'].items():
                        if region not in all_regions:
                            all_regions[region] = 0
                        all_regions[region] += stats.get('æ€»é”€å”®é¢', 0)
                
                # äº§å“ç»Ÿè®¡
                if 'top_products' in summary:
                    for product, stats in summary['top_products'].items():
                        if product not in all_products:
                            all_products[product] = 0
                        all_products[product] += stats.get('æ€»é”€å”®é¢', 0)
        
        # æ’åº
        top_regions = dict(sorted(all_regions.items(), key=lambda x: x[1], reverse=True)[:10])
        top_products = dict(sorted(all_products.items(), key=lambda x: x[1], reverse=True)[:10])
        
        # æ—¥æœŸèŒƒå›´
        date_range = None
        if self.combined_data is not None and 'Order_Date' in self.combined_data.columns:
            try:
                dates = pd.to_datetime(self.combined_data['Order_Date'], errors='coerce').dropna()
                if len(dates) > 0:
                    date_range = {
                        'start': dates.min().strftime('%Y-%m-%d'),
                        'end': dates.max().strftime('%Y-%m-%d')
                    }
            except:
                pass
        
        return {
            'total_files_processed': success_files,
            'total_records': total_records,
            'total_sales': total_sales,
            'avg_order_value': total_sales / total_records if total_records > 0 else 0,
            'top_regions': top_regions,
            'top_products': top_products,
            'file_breakdown': file_breakdown,
            'date_range': date_range
        } 